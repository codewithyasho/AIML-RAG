{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6cbd239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader, WebBaseLoader, DirectoryLoader, PyPDFLoader\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84b4fa1",
   "metadata": {},
   "source": [
    "## loading all files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962f51a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load all text files from the directory\n",
    "# loader = DirectoryLoader(\n",
    "#     path=\"./data/text_data\",\n",
    "#     glob=\"**/*.txt\",\n",
    "#     loader_cls=TextLoader,\n",
    "#     loader_kwargs={\"encoding\": \"utf-8\"},\n",
    "#     show_progress=True\n",
    "\n",
    "# )\n",
    "\n",
    "# text_documents = loader.load()\n",
    "\n",
    "# text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e572b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Number of documents: {len(text_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e7582ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # web loader\n",
    "# web_loader = WebBaseLoader([\n",
    "#     \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "#     \"https://en.wikipedia.org/wiki/Machine_learning\",\n",
    "#     \"https://en.wikipedia.org/wiki/Deep_learning\",\n",
    "#     \"https://en.wikipedia.org/wiki/Natural_language_processing\",\n",
    "#     \"https://en.wikipedia.org/wiki/Computer_vision\"\n",
    "# ])\n",
    "# web_docs = web_loader.load()\n",
    "\n",
    "# web_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7716dcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(web_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b3b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "# pdf_loader = PyPDFDirectoryLoader(\n",
    "#     path=\"./data/pdf_data\",\n",
    "#     glob=\"**/[!.]*.pdf\",  # Default pattern\n",
    "#     silent_errors=False,\n",
    "#     recursive=False,\n",
    "#     extract_images=True\n",
    "# )\n",
    "\n",
    "# pdf_documents = pdf_loader.load()\n",
    "# pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "90dd6968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(pdf_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52edc8f0",
   "metadata": {},
   "source": [
    "## splitting all documents into chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "07125154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# # Combine all documents into one list\n",
    "# all_documents = text_documents + pdf_documents + web_docs\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=1000,\n",
    "#     chunk_overlap=200\n",
    "# )\n",
    "\n",
    "\n",
    "# all_chunks = text_splitter.split_documents(all_documents)\n",
    "\n",
    "# print(f\"Total chunks: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63f66b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e862fd23",
   "metadata": {},
   "source": [
    "## creating vector embeddings and storing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f1a0c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89922ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=\"sentence-transformers/all-MiniLM-L12-v2\",\n",
    "#     model_kwargs={\"device\": \"cpu\"},\n",
    "\n",
    "#     encode_kwargs={\n",
    "#         \"normalize_embeddings\": True,  # ADD THIS - Important for similarity!\n",
    "#         \"batch_size\": 32  # Optional but recommended\n",
    "#     },\n",
    "\n",
    "#     cache_folder=\"./models\",\n",
    "#     show_progress=True\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4fdcdb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"mxbai-embed-large:latest\",\n",
    "    base_url=\"http://localhost:11434\",  # Ollama server URL\n",
    "    num_thread=10  # Number of threads for processing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7e1653c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings(model='mxbai-embed-large:latest', validate_model_on_init=False, base_url='http://localhost:11434', client_kwargs={}, async_client_kwargs={}, sync_client_kwargs={}, mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gpu=None, keep_alive=None, num_thread=10, repeat_last_n=None, repeat_penalty=None, temperature=None, stop=None, tfs_z=None, top_k=None, top_p=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a1f3d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore = Chroma.from_documents(\n",
    "#     documents=all_chunks,\n",
    "#     embedding=embeddings,\n",
    "#     collection_name=\"rag_collection\",\n",
    "#     persist_directory=\"./chroma_db\"  # Saves to disk\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4c5ea462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load existing vector store (NO re-embedding needed!)\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"rag_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_db\"  # Points to existing folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "28c894f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x21f1e58e5d0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9987cfd",
   "metadata": {},
   "source": [
    "## creating retriver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "633bea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "# Use in your RAG pipeline\n",
    "# retrieved_docs = retriever.invoke(\"What is deep learning?\")\n",
    "# len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5b092f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieved_docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc8e6de",
   "metadata": {},
   "source": [
    "## llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "556fdbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-120b\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "61605cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context. \n",
    "Think step by step before answering.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4bda31",
   "metadata": {},
   "source": [
    "## building chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c2c04ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ac194b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3418ade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Step‑by‑step reasoning**\n",
      "\n",
      "1. The excerpt talks about “our data” and then immediately moves to the most common structure used to *wrangle* data – the **data frame**.  \n",
      "2. It describes a data frame as *tabular*, meaning it is organized in **rows and columns** just like a spreadsheet.  \n",
      "3. An example is shown: a Titanic passenger data frame that contains fields such as **Name, PClass, Age, Sex, Survived, SexCode**. Each row represents one passenger (an observation) and each column represents a particular attribute (a variable).  \n",
      "4. From this description we can infer that **data** is the collection of factual information (values) that can be stored, displayed, and processed in such a structured form.\n",
      "\n",
      "**Answer**\n",
      "\n",
      "Data is the collection of factual information—values or observations—about something. In practice it is often organized in a tabular structure called a *data frame*, where each **row** represents an individual record (e.g., a passenger) and each **column** represents a specific attribute or variable (e.g., age, sex, survival status). This structured arrangement lets us view, clean, and analyze the information efficiently.\n"
     ]
    }
   ],
   "source": [
    "result = rag_chain.invoke(\n",
    "    input={\"input\": \"What is data?\"}\n",
    ")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc996b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bc2a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
